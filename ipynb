# Title: NSAP Eligibility Prediction

# Step 1: Setup & Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pickle
import os

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 180)

print('Libraries imported successfully.')

# Step 2: Load dataset
CSV_FILENAME = "nsap_dataset.csv"
TARGET_COL = "Eligible"  # Change if your target column has a different name

if not os.path.exists(CSV_FILENAME):
    print(f"⚠️ File '{CSV_FILENAME}' not found. Upload it and re-run.")
else:
    df = pd.read_csv(CSV_FILENAME)
    print(f"✅ Loaded '{CSV_FILENAME}' — shape:", df.shape)
    display(df.head())

# Step 3: Data info & cleaning
print('--- Info ---')
display(df.info())
print('\n--- Description ---')
display(df.describe())
print('\nDuplicates before:', df.duplicated().sum())
df.drop_duplicates(inplace=True)
print('Duplicates after:', df.duplicated().sum())

# Fill numeric NaN with mean and categorical with mode
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
obj_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
df[num_cols] = df[num_cols].fillna(df[num_cols].mean())

for c in obj_cols:
    df[c] = df[c].fillna(df[c].mode().iloc[0] if not df[c].mode().empty else '')

print('Missing values handled.')

# Step 4: EDA
num_df = df.select_dtypes(include=[np.number])
if num_df.shape[1] >= 2:
    plt.figure(figsize=(10,8))
    sns.heatmap(num_df.corr(), annot=True, fmt='.2f')
    plt.title('Correlation Heatmap')
    plt.show()

if TARGET_COL in df.columns:
    print('Class distribution:')
    display(df[TARGET_COL].value_counts())
    sns.countplot(x=TARGET_COL, data=df)
    plt.title('Target Class Distribution')
    plt.show()

# Step 5: Preprocessing
X = df.drop(columns=[TARGET_COL])
y = df[TARGET_COL].copy()

if y.dtype == 'object' or str(y.dtype).startswith('category'):
    y = y.astype('category').cat.codes
    print('Target encoded to numeric codes.')

X = pd.get_dummies(X, drop_first=True)
print('Features shape:', X.shape)
display(X.head())

# Step 6: Train-test split & Model Training
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y if len(y.unique())>1 else None
)
print('Train:', X_train.shape, 'Test:', X_test.shape)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
print('✅ Model trained.')

# Step 7: Evaluation
y_pred = model.predict(X_test)
print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')
print('\nClassification Report:\n', classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Step 8: Feature Importance
importances = pd.Series(model.feature_importances_, index=X.columns)
top_n = min(20, len(importances))
importances.sort_values(ascending=True).tail(top_n).plot(kind='barh', figsize=(8,5))
plt.title(f'Top {top_n} Feature Importances')
plt.show()

# Step 9: Save model
MODEL_FILENAME = 'nsap_model.pkl'
with open(MODEL_FILENAME, 'wb') as f:
    pickle.dump(model, f)
print(f'Model saved
